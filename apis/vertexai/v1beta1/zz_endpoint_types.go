/*
Copyright 2021 The Crossplane Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

// Code generated by upjet. DO NOT EDIT.

package v1beta1

import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime/schema"

	v1 "github.com/crossplane/crossplane-runtime/apis/common/v1"
)

type AutomaticResourcesObservation struct {

	// The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
	MaxReplicaCount *float64 `json:"maxReplicaCount,omitempty" tf:"max_replica_count,omitempty"`

	// The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
	MinReplicaCount *float64 `json:"minReplicaCount,omitempty" tf:"min_replica_count,omitempty"`
}

type AutomaticResourcesParameters struct {
}

type AutoscalingMetricSpecsObservation struct {

	// The resource metric name. Supported metrics: * For Online Prediction: * aiplatform.googleapis.com/prediction/online/accelerator/duty_cycle * aiplatform.googleapis.com/prediction/online/cpu/utilization
	MetricName *string `json:"metricName,omitempty" tf:"metric_name,omitempty"`

	// The target resource utilization in percentage (1% - 100%) for the given metric; once the real usage deviates from the target by a certain percentage, the machine replicas change. The default value is 60 (representing 60%) if not provided.
	Target *float64 `json:"target,omitempty" tf:"target,omitempty"`
}

type AutoscalingMetricSpecsParameters struct {
}

type DedicatedResourcesObservation struct {

	// The metric specifications that overrides a resource utilization metric (CPU utilization, accelerator's duty cycle, and so on) target value (default to 60 if not set). At most one entry is allowed per metric. If machine_spec.accelerator_count is above 0, the autoscaling will be based on both CPU utilization and accelerator's duty cycle metrics and scale up when either metrics exceeds its target value while scale down if both metrics are under their target value. The default target value is 60 for both metrics. If machine_spec.accelerator_count is 0, the autoscaling will be based on CPU utilization metric only with default target value 60 if not explicitly set. For example, in the case of Online Prediction, if you want to override target CPU utilization to 80, you should set autoscaling_metric_specs.metric_name to aiplatform.googleapis.com/prediction/online/cpu/utilization and autoscaling_metric_specs.target to 80.
	// Structure is documented below.
	AutoscalingMetricSpecs []AutoscalingMetricSpecsObservation `json:"autoscalingMetricSpecs,omitempty" tf:"autoscaling_metric_specs,omitempty"`

	// The specification of a single machine used by the prediction.
	// Structure is documented below.
	MachineSpec []MachineSpecObservation `json:"machineSpec,omitempty" tf:"machine_spec,omitempty"`

	// The maximum number of replicas this DeployedModel may be deployed on when the traffic against it increases. If the requested value is too large, the deployment will error, but if deployment succeeds then the ability to scale the model to that many replicas is guaranteed (barring service outages). If traffic against the DeployedModel increases beyond what its replicas at maximum may handle, a portion of the traffic will be dropped. If this value is not provided, will use min_replica_count as the default value. The value of this field impacts the charge against Vertex CPU and GPU quotas. Specifically, you will be charged for max_replica_count * number of cores in the selected machine type) and (max_replica_count * number of GPUs per replica in the selected machine type).
	MaxReplicaCount *float64 `json:"maxReplicaCount,omitempty" tf:"max_replica_count,omitempty"`

	// The minimum number of machine replicas this DeployedModel will be always deployed on. This value must be greater than or equal to 1. If traffic against the DeployedModel increases, it may dynamically be deployed onto more replicas, and as traffic decreases, some of these extra replicas may be freed.
	MinReplicaCount *float64 `json:"minReplicaCount,omitempty" tf:"min_replica_count,omitempty"`
}

type DedicatedResourcesParameters struct {
}

type DeployedModelsObservation struct {

	// A description of resources that to large degree are decided by Vertex AI, and require only a modest additional configuration.
	// Structure is documented below.
	AutomaticResources []AutomaticResourcesObservation `json:"automaticResources,omitempty" tf:"automatic_resources,omitempty"`

	// Output only. Timestamp when the DeployedModel was created.
	CreateTime *string `json:"createTime,omitempty" tf:"create_time,omitempty"`

	// A description of resources that are dedicated to the DeployedModel, and that need a higher degree of manual configuration.
	// Structure is documented below.
	DedicatedResources []DedicatedResourcesObservation `json:"dedicatedResources,omitempty" tf:"dedicated_resources,omitempty"`

	// The display name of the DeployedModel. If not provided upon creation, the Model's display_name is used.
	DisplayName *string `json:"displayName,omitempty" tf:"display_name,omitempty"`

	// These logs are like standard server access logs, containing information like timestamp and latency for each prediction request. Note that Stackdriver logs may incur a cost, especially if your project receives prediction requests at a high queries per second rate (QPS). Estimate your costs before enabling this option.
	EnableAccessLogging *bool `json:"enableAccessLogging,omitempty" tf:"enable_access_logging,omitempty"`

	// If true, the container of the DeployedModel instances will send stderr and stdout streams to Stackdriver Logging. Only supported for custom-trained Models and AutoML Tabular Models.
	EnableContainerLogging *bool `json:"enableContainerLogging,omitempty" tf:"enable_container_logging,omitempty"`

	// The ID of the DeployedModel. If not provided upon deployment, Vertex AI will generate a value for this ID. This value should be 1-10 characters, and valid characters are /[0-9]/.
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// The name of the Model that this is the deployment of. Note that the Model may be in a different location than the DeployedModel's Endpoint.
	Model *string `json:"model,omitempty" tf:"model,omitempty"`

	// Output only. The version ID of the model that is deployed.
	ModelVersionID *string `json:"modelVersionId,omitempty" tf:"model_version_id,omitempty"`

	// Output only. Provide paths for users to send predict/explain/health requests directly to the deployed model services running on Cloud via private services access. This field is populated if network is configured.
	// Structure is documented below.
	PrivateEndpoints []PrivateEndpointsObservation `json:"privateEndpoints,omitempty" tf:"private_endpoints,omitempty"`

	// The service account that the DeployedModel's container runs as. Specify the email address of the service account. If this service account is not specified, the container runs as a service account that doesn't have access to the resource project. Users deploying the Model must have the iam.serviceAccounts.actAs permission on this service account.
	ServiceAccount *string `json:"serviceAccount,omitempty" tf:"service_account,omitempty"`

	// The resource name of the shared DeploymentResourcePool to deploy on. Format: projects/{project}/locations/{location}/deploymentResourcePools/{deployment_resource_pool}
	SharedResources *string `json:"sharedResources,omitempty" tf:"shared_resources,omitempty"`
}

type DeployedModelsParameters struct {
}

type EndpointEncryptionSpecObservation struct {
}

type EndpointEncryptionSpecParameters struct {

	// Required. The Cloud KMS resource identifier of the customer managed encryption key used to protect a resource. Has the form: projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key. The key needs to be in the same region as where the compute resource is created.
	// +kubebuilder:validation:Required
	KMSKeyName *string `json:"kmsKeyName" tf:"kms_key_name,omitempty"`
}

type EndpointObservation struct {

	// Output only. Timestamp when this Endpoint was created.
	CreateTime *string `json:"createTime,omitempty" tf:"create_time,omitempty"`

	// Output only. The models deployed in this Endpoint. To add or remove DeployedModels use EndpointService.DeployModel and EndpointService.UndeployModel respectively. Models can also be deployed and undeployed using the Cloud Console.
	// Structure is documented below.
	DeployedModels []DeployedModelsObservation `json:"deployedModels,omitempty" tf:"deployed_models,omitempty"`

	// Used to perform consistent read-modify-write updates. If not set, a blind "overwrite" update happens.
	Etag *string `json:"etag,omitempty" tf:"etag,omitempty"`

	// an identifier for the resource with format projects/{{project}}/locations/{{location}}/endpoints/{{name}}
	ID *string `json:"id,omitempty" tf:"id,omitempty"`

	// Output only. Resource name of the Model Monitoring job associated with this Endpoint if monitoring is enabled by CreateModelDeploymentMonitoringJob. Format: projects/{project}/locations/{location}/modelDeploymentMonitoringJobs/{model_deployment_monitoring_job}
	ModelDeploymentMonitoringJob *string `json:"modelDeploymentMonitoringJob,omitempty" tf:"model_deployment_monitoring_job,omitempty"`

	// Output only. Timestamp when this Endpoint was last updated.
	UpdateTime *string `json:"updateTime,omitempty" tf:"update_time,omitempty"`
}

type EndpointParameters struct {

	// The description of the Endpoint.
	// +kubebuilder:validation:Optional
	Description *string `json:"description,omitempty" tf:"description,omitempty"`

	// Required. The display name of the Endpoint. The name can be up to 128 characters long and can consist of any UTF-8 characters.
	// +kubebuilder:validation:Required
	DisplayName *string `json:"displayName" tf:"display_name,omitempty"`

	// Customer-managed encryption key spec for an Endpoint. If set, this Endpoint and all sub-resources of this Endpoint will be secured by this key.
	// Structure is documented below.
	// +kubebuilder:validation:Optional
	EncryptionSpec []EndpointEncryptionSpecParameters `json:"encryptionSpec,omitempty" tf:"encryption_spec,omitempty"`

	// The labels with user-defined metadata to organize your Endpoints. Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed. See https://goo.gl/xmQnxf for more information and examples of labels.
	// +kubebuilder:validation:Optional
	Labels map[string]*string `json:"labels,omitempty" tf:"labels,omitempty"`

	// The location for the resource
	// +kubebuilder:validation:Required
	Location *string `json:"location" tf:"location,omitempty"`

	// The full name of the Google Compute Engine network to which the Endpoint should be peered. Private services access must already be configured for the network. If left unspecified, the Endpoint is not peered with any network. Only one of the fields, network or enable_private_service_connect, can be set. Format: projects/{project}/global/networks/{network}. Where {project} is a project number, as in 12345, and {network} is network name.
	// +kubebuilder:validation:Optional
	Network *string `json:"network,omitempty" tf:"network,omitempty"`

	// The ID of the project in which the resource belongs.
	// If it is not provided, the provider project is used.
	// +kubebuilder:validation:Optional
	Project *string `json:"project,omitempty" tf:"project,omitempty"`
}

type MachineSpecObservation struct {

	// The number of accelerators to attach to the machine.
	AcceleratorCount *float64 `json:"acceleratorCount,omitempty" tf:"accelerator_count,omitempty"`

	// The type of accelerator(s) that may be attached to the machine as per accelerator_count. See possible values here.
	AcceleratorType *string `json:"acceleratorType,omitempty" tf:"accelerator_type,omitempty"`

	// The type of the machine. See the list of machine types supported for prediction See the list of machine types supported for custom training. For DeployedModel this field is optional, and the default value is n1-standard-2. For BatchPredictionJob or as part of WorkerPoolSpec this field is required. TODO(rsurowka): Try to better unify the required vs optional.
	MachineType *string `json:"machineType,omitempty" tf:"machine_type,omitempty"`
}

type MachineSpecParameters struct {
}

type PrivateEndpointsObservation struct {

	// Output only. Http(s) path to send explain requests.
	ExplainHTTPURI *string `json:"explainHttpUri,omitempty" tf:"explain_http_uri,omitempty"`

	// Output only. Http(s) path to send health check requests.
	HealthHTTPURI *string `json:"healthHttpUri,omitempty" tf:"health_http_uri,omitempty"`

	// Output only. Http(s) path to send prediction requests.
	PredictHTTPURI *string `json:"predictHttpUri,omitempty" tf:"predict_http_uri,omitempty"`

	// Output only. The name of the service attachment resource. Populated if private service connect is enabled.
	ServiceAttachment *string `json:"serviceAttachment,omitempty" tf:"service_attachment,omitempty"`
}

type PrivateEndpointsParameters struct {
}

// EndpointSpec defines the desired state of Endpoint
type EndpointSpec struct {
	v1.ResourceSpec `json:",inline"`
	ForProvider     EndpointParameters `json:"forProvider"`
}

// EndpointStatus defines the observed state of Endpoint.
type EndpointStatus struct {
	v1.ResourceStatus `json:",inline"`
	AtProvider        EndpointObservation `json:"atProvider,omitempty"`
}

// +kubebuilder:object:root=true

// Endpoint is the Schema for the Endpoints API. Models are deployed into it, and afterwards Endpoint is called to obtain predictions and explanations.
// +kubebuilder:printcolumn:name="READY",type="string",JSONPath=".status.conditions[?(@.type=='Ready')].status"
// +kubebuilder:printcolumn:name="SYNCED",type="string",JSONPath=".status.conditions[?(@.type=='Synced')].status"
// +kubebuilder:printcolumn:name="EXTERNAL-NAME",type="string",JSONPath=".metadata.annotations.crossplane\\.io/external-name"
// +kubebuilder:printcolumn:name="AGE",type="date",JSONPath=".metadata.creationTimestamp"
// +kubebuilder:subresource:status
// +kubebuilder:resource:scope=Cluster,categories={crossplane,managed,gcp}
type Endpoint struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec              EndpointSpec   `json:"spec"`
	Status            EndpointStatus `json:"status,omitempty"`
}

// +kubebuilder:object:root=true

// EndpointList contains a list of Endpoints
type EndpointList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Endpoint `json:"items"`
}

// Repository type metadata.
var (
	Endpoint_Kind             = "Endpoint"
	Endpoint_GroupKind        = schema.GroupKind{Group: CRDGroup, Kind: Endpoint_Kind}.String()
	Endpoint_KindAPIVersion   = Endpoint_Kind + "." + CRDGroupVersion.String()
	Endpoint_GroupVersionKind = CRDGroupVersion.WithKind(Endpoint_Kind)
)

func init() {
	SchemeBuilder.Register(&Endpoint{}, &EndpointList{})
}
